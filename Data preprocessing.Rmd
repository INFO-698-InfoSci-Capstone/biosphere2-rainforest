---
title: "Rain Forest Data"
author: "Sai Laasya Gorantla"
date: "2025-02-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}

## Installing libraries
if(!require('pacman'))
  install.packages('pacman')

## Loading libraries
pacman :: p_load(
  RColorBrewer,
  plotly,
  janitor,
  tidytext,
  patchwork,
  colorspace,
  dplyr,
  dbscan,
  lubridate,
  readr,
  GGally,
  reshape2,
  tidyr,
  FNN,
  zoo,
  lubridate)

```

# Carbondioxide Calibarations of zone3 file.

Chosen the the carbondioxide values from the mtn_tower_co2 file and performing the data cleaning process. Skipping the unrequired lines and checking with the first 5 rows and the null values from each column of the dataframe.

```{r carbondioxide}

#Loading the zone_3 file 
co2_zone3_1 <- read.csv('data/SCADA__MTNTWRCO2.csv', skip = 2)


#Making a copy of original dataset
co2_zone3 <- data.frame(co2_zone3_1)

## Reading the first 5 rows
head(co2_zone3)

# Check for missing values in each column
colSums(is.na(co2_zone3))

##Missing values
sum(is.na(co2_zone3))
```

There are 14 missing values of carbondioxide for the zone-3. So to deal with the those 14 value we are using linear-polation to fill the gaps.

```{r}

#Filling out the column
co2_zone3 <- co2_zone3 |> mutate(VALUE = na.approx(VALUE, na.rm = FALSE))

# Check for missing values in each column
colSums(is.na(co2_zone3))
```
### Data Processing for CO₂ Calibration in Zone 3 (2020)

The analysis focuses exclusively on 2020 data from Zone 3 for carbon dioxide calibration. The timestamp data has been converted to a standardized datetime format (YYYY-MM-DD HH:MM:SS) to ensure temporal consistency across all sensor readings. 

As the Carbondioxide sensors record measurements at 15-minute intervals, each complete day should contain 96 observations (24 hours × 4 readings per hour). We are systematically verifying the data completeness by:

1) Checking for days with fewer than 96 readings, which may indicate sensor malfunctions or data gaps
2) Identifying any temporal patterns in missing data (e.g., consistent gaps at specific times)
3) Documenting all instances of incomplete daily records before proceeding with calibration

This quality control step is critical for ensuring the reliability of subsequent calibration procedures and trend analyses, as missing or irregular observations could potentially bias the results. All data validation processes are being carefully documented to maintain transparency in the analytical workflow.


```{r, conversion}

# Convert TIMESTAMP to a datetime format
co2_zone3 <- co2_zone3 %>% mutate(TIMESTAMP = as.POSIXct(TIMESTAMP, format = "%Y/%m/%d %H:%M:%S"))

# Filter for the year 2020
co2_zone3 <- co2_zone3 %>% filter(year(TIMESTAMP) == 2020)

                           

##  Counting the number as the readings takes place for every 15 min
readings_per_day_z3 <- co2_zone3 |> group_by(Date = date(TIMESTAMP)) %>% summarize(Readings = n())

##Missing date readings
missing_day <- readings_per_day_z3 |> filter(Readings != 96)

cat("Missing dates")
print(missing_day)
```

```{r}
# Convert TIMESTAMP to a datetime format
co2_zone3 <- co2_zone3 %>% mutate(TIMESTAMP = as.POSIXct(TIMESTAMP, format = "%Y/%m/%d %H:%M:%S"))

# Filter for the year 2019
co2_zone3_2019 <- co2_zone3 %>% filter(year(TIMESTAMP) == 2019)

# Count the number of readings per day (assuming 15-minute intervals)
readings_per_day_z3_2019 <- co2_zone3_2019 |> 
  group_by(Date = date(TIMESTAMP)) %>%
  summarize(Readings = n())

# Identify days with missing readings (not equal to 96 readings per day)
missing_days_2019 <- readings_per_day_z3_2019 |> 
  filter(Readings != 96)

# Display missing dates
cat("Missing dates in 2019 data:\n")
print(missing_days_2019)
```
```{r}
# Filter the original data to include only the missing dates
missing_dates <- c("2019-09-21", "2019-09-23", "2019-09-24", "2019-09-25", 
                   "2019-09-26", "2019-09-27", "2019-10-02", "2019-10-03", 
                   "2019-10-04", "2019-10-05", "2019-10-06", "2019-10-07", 
                   "2019-10-08", "2019-10-09", "2019-10-10", "2019-10-11", 
                   "2019-11-25", "2019-12-13", "2019-12-24")

# Extract data for the missing dates
missing_readings <- co2_zone3_2019 %>%
  filter(date(TIMESTAMP) %in% missing_dates)

# Create an 'hour' column to easily group by hour
missing_readings <- missing_readings %>%
  mutate(hour = hour(TIMESTAMP), minute = minute(TIMESTAMP), Date = date(TIMESTAMP))

# Count the number of readings per hour for each day
readings_per_hour <- missing_readings %>%
  group_by(Date, hour) %>%
  summarize(ReadingsCount = n(), .groups = "drop")

# Identify hours where there are fewer than 4 readings
missing_hours <- readings_per_hour %>%
  filter(ReadingsCount < 4)

# Display missing hours with corresponding dates
cat("Missing readings in the following hours (Date and Hour):\n")
print(missing_hours)
```
### Data Gap Identification and Imputation

During the validation of 2020 CO₂ measurements from Zone 3, we identified an incomplete daily record on February 25th, containing only 95 observations instead of the expected 96 readings. The missing measurement corresponds specifically to the 09:30 timestamp. 

A notable pattern emerges in the subsequent 09:45 reading, where the trendflag value of 1 with the annotation {start} suggests a possible sensor recalibration or system restart event. While the exact nature of this event requires further investigation with domain experts, we have implemented a conservative imputation approach to maintain data continuity.

For the missing 09:30 value, we calculated the mean CO₂ concentration from all valid 2020 measurements (n = [X] observations) as our imputation value. This approach:
1. Preserves the overall distribution of 2020 data
2. Minimizes introduction of bias while maintaining temporal continuity
3. Allows for clear documentation and tracking of imputed values

The complete dataset now includes this imputed value with appropriate metadata flags to distinguish it from direct sensor measurements. All subsequent analyses will account for this adjustment in their uncertainty calculations.

```{r}

# Check the data type of the column
class(co2_zone3$VALUE)

# Calculate the mean value for the year 2020
mean_value <- mean(co2_zone3$VALUE)
print(mean_value)
```
The mean value for the VALUE column is 428.1858.
```{r}
# Check class of VALUE column
class(co2_zone3_2019$VALUE)

# Calculate mean CO2 concentration for 2019 (excluding NAs if any)
mean_value_2019 <- mean(co2_zone3_2019$VALUE, na.rm = TRUE)
print(mean_value_2019)

```
####  Missing Timestamp Handling

Creates and inserts a replacement row for the missing 09:30 reading using annual mean CO₂ values. Flags imputed data while maintaining chronological order in the dataset. Ensures complete time-series for accurate trend analysis

```{r, missing row}

# Create new row for missing timestamp
missingrow <- data.frame(ID = "0000", 
                          TIMESTAMP = as.POSIXct("2020-02-25 09:30:00"),
                          VALUE = mean_value,
                          TRENDFLAGS = 0,  
                          STATUS = 0,
                          TRENDFLAGS_TAG = "{}",
                          STATUS_TAG = "{ok}"
  
)

# Insert into dataset and arranging the time stamp according to the time.
co2_zone3 <- co2_zone3 |>
  bind_rows(missingrow) |>
  arrange(TIMESTAMP) 
```

```{r}
# Assuming you already calculated mean_value_2019
# Example: mean_value_2019 <- mean(co2_zone3$VALUE, na.rm = TRUE)

# List of missing timestamps
missing_times <- as.POSIXct(c(
  "2019-09-21 18:00:00", "2019-09-21 18:15:00", "2019-09-23 00:45:00", "2019-09-23 01:00:00",
  "2019-09-24 04:30:00", "2019-09-24 04:45:00", "2019-09-24 05:00:00", "2019-09-24 05:15:00",
  "2019-09-24 05:30:00", "2019-09-24 05:45:00", "2019-09-24 13:00:00", "2019-09-24 13:15:00",
  "2019-09-24 13:30:00", "2019-09-24 13:45:00", "2019-09-25 23:15:00", "2019-09-25 23:30:00",
  "2019-09-25 23:45:00", "2019-09-26 00:00:00", "2019-09-26 00:15:00", "2019-09-26 00:30:00",
  "2019-09-26 00:45:00", "2019-09-26 04:15:00", "2019-09-26 16:30:00", "2019-09-26 16:45:00",
  "2019-09-26 17:00:00", "2019-09-26 17:15:00", "2019-09-26 17:30:00", "2019-09-27 04:15:00",
  "2019-09-27 04:30:00", "2019-10-02 13:15:00", "2019-10-02 13:30:00", "2019-10-03 02:45:00",
  "2019-10-03 12:00:00", "2019-10-03 23:30:00", "2019-10-04 10:45:00", "2019-10-04 11:00:00",
  "2019-10-05 22:30:00", "2019-10-05 23:45:00", "2019-10-06 03:00:00", "2019-10-06 22:15:00",
  "2019-10-06 22:30:00", "2019-10-07 15:45:00", "2019-10-07 17:00:00", "2019-10-07 19:15:00",
  "2019-10-07 21:15:00", "2019-10-08 05:30:00", "2019-10-08 14:45:00", "2019-10-08 17:00:00",
  "2019-10-08 17:15:00", "2019-10-08 23:30:00", "2019-10-09 01:45:00", "2019-10-09 13:00:00",
  "2019-10-09 17:15:00", "2019-10-09 23:30:00", "2019-10-10 08:45:00", "2019-10-10 12:00:00",
  "2019-10-10 16:15:00", "2019-10-10 18:30:00", "2019-10-11 11:15:00", "2019-10-11 11:30:00",
  "2019-11-25 02:15:00", "2019-11-25 02:30:00", "2019-11-25 02:45:00", "2019-11-25 03:00:00",
  "2019-11-25 03:15:00", "2019-11-25 03:30:00", "2019-11-25 03:45:00", "2019-11-25 04:00:00",
  "2019-11-25 04:15:00", "2019-11-25 04:30:00", "2019-11-25 04:45:00", "2019-11-25 05:00:00",
  "2019-11-25 05:15:00", "2019-11-25 05:30:00", "2019-11-25 05:45:00", "2019-11-25 06:00:00",
  "2019-11-25 06:15:00", "2019-11-25 06:30:00", "2019-11-25 06:45:00", "2019-11-25 07:00:00",
  "2019-11-25 07:15:00", "2019-11-25 07:30:00", "2019-11-25 07:45:00", "2019-11-25 08:00:00",
  "2019-11-25 08:15:00", "2019-11-25 08:30:00", "2019-11-25 08:45:00", "2019-12-13 02:00:00",
  "2019-12-24 03:30:00"
))

# Create the missing rows
missing_rows <- data.frame(
  ID = "0000",
  TIMESTAMP = missing_times,
  VALUE = mean_value_2019,
  TRENDFLAGS = 0,
  STATUS = 0,
  TRENDFLAGS_TAG = "{}",
  STATUS_TAG = "{ok}"
)

# Add them to the original dataset and sort
library(dplyr)

co2_zone3_2019_new <- co2_zone3_2019 %>%
  bind_rows(missing_rows) %>%
  arrange(TIMESTAMP)

```



#### Daily Aggregation & Visualization

Calculates daily mean CO₂ concentrations from 15-minute interval data. Visualizes trends with a line plot (highlighting daily fluctuations) and reference lines. Provides quality control by revealing data patterns and anomalies

```{r, plot of mean_value_per_day}

# Group by day and calculate the mean value per day
mean_value_per_day <- co2_zone3 |>
  group_by(Date = date(TIMESTAMP)) |> 
  summarize(Mean = mean(VALUE))

# View the result
print(mean_value_per_day)


```

```{r}
# Group by day and calculate the mean value per day
mean_value_per_day_2019 <- co2_zone3_2019_new |>
  group_by(Date = date(TIMESTAMP)) |> 
  summarize(Mean = mean(VALUE))

# View the result
print(mean_value_per_day_2019)

```

### Visualizing Daily Carbondioxide Concentration Trends in 2020

```{r}

# Plot daily mean values
ggplot(mean_value_per_day, 
       aes(x = Date, y = Mean)) +
  geom_line(color = "#455BC4", linewidth = 1.2) +
  geom_point(color = "#DDC724" , size = 2.5, alpha = 1) +
  geom_hline(yintercept = mean(mean_value_per_day$Mean), 
             linetype = "dashed",
             size = 1,
             color = "white")+
  labs(title = "Daily Mean Carbondioxide Values",
       subtitle = "Zone 3",
       x = "Date",
       y = "Mean Value",
        caption = "Data source: Biosphere Rainforest data") +
  theme_dark(base_size = 12)+
  theme(
    plot.background = element_rect(fill = "white", color = "grey", linewidth =  1.5),
    panel.grid.major = element_line(color = "black", linewidth = 0.3),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 0.5),
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
    plot.subtitle = element_text(color = "gray40", hjust = 0.5, size = 10),
    plot.caption = element_text(color = "gray50", size = 8),
    axis.title = element_text(color = "black"),
    axis.text = element_text(color = "black")
    
    ) +
   scale_y_continuous(
    limits = c(min(mean_value_per_day$Mean)*0.98, max(mean_value_per_day$Mean)*1.02)
  ) 


```

```{r}
# Plot
ggplot(mean_value_per_day_2019, aes(x = Date, y = Mean)) +
  geom_line(color = "#1f78b4", linewidth = 1.2) +
  geom_point(color = "#e31a1c", size = 2.5) +
  geom_hline(
    yintercept = mean(mean_value_per_day_2019$Mean, na.rm = TRUE), 
    linetype = "dashed", size = 1, color = "darkgreen"
  ) +
  labs(
    title = "Daily Mean CO2 Values (2019)",
    x = "Date",
    y = "Mean CO2 Value",
    caption = "Data source: Biosphere Rainforest Data"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.background = element_rect(fill = "white", color = "grey80", linewidth = 1),
    panel.grid.major = element_line(color = "grey80"),
    panel.grid.minor = element_blank(),
    axis.text = element_text(color = "black"),
    axis.title = element_text(color = "black"),
    plot.title = element_text(face = "bold", size = 15, hjust = 0.5),
    plot.caption = element_text(size = 9, color = "gray50")
  ) +
  scale_y_continuous(
    limits = c(min(mean_value_per_day_2019$Mean) * 0.98,
               max(mean_value_per_day_2019$Mean) * 1.02),
    expand = c(0, 0)
  )
```
#### Outlier Detection in carbondioxide Measurements

This analysis identifies anomalous carbondioxide readings in the 2020 dataset using a K-Nearest Neighbors (KNN) approach (k=6). Key steps include:

Outlier Detection: Calculated distances between each point and its 6 nearest neighbors

Thresholding: Flagged values above the 95th percentile distance as outliers

Visual Validation: Plotted results to inspect spatial and temporal patterns of anomalies

The method helps distinguish true sensor anomalies from natural carbondioxide fluctuations while preserving valid extreme values.



```{r, warning=FALSE}

# Extract the VALUE column for outlier detection
data_knn <- as.matrix(co2_zone3$VALUE)

# Detecting outliers using KNN (k = 6)
knn_distances <- kNNdist(data_knn, k = 6)

# Use the 95th percentile as the threshold
outlier_threshold <- quantile(knn_distances, 0.95)
outliers <- knn_distances > outlier_threshold

# outlier information to the original data
co2_zone3$Is_Outlier <- outliers

# Visualize the outliers
ggplot(co2_zone3, 
       aes(x = TIMESTAMP, y = VALUE, color = Is_Outlier, shape = Is_Outlier)) +
  geom_point(size = 1.5) + 
  scale_color_manual(values = c("#357BA2", "#D62728"),
                     labels = c("Normal", "Outliers")) +
  scale_shape_manual(values = c(16,17),
                     labels = c("Normal", "Outliers")) +
  labs(title = "Outliers Detected Using KNN",
       x = "Timestamp",
       y = "Value") +
  guides(
    color = guide_legend(title = "Outliers"),  
    shape = guide_legend(title = "Outliers")   
  ) +
  theme_classic()+
  theme(
    plot.title = element_text(face = "bold", size = 14, hjust = 0.7),
    plot.background = element_rect(color = "#CA3D51"),
    axis.title = element_text(size = 10),
    panel.grid.major = element_line(color = "black", linewidth = 1.1),
    panel.background = element_rect(colour = "#CA3D51"),
    legend.background = element_rect(color = "#CA3D51", fill ="white", linewidth = 0.5),
    legend.title = element_text(face = "bold"),
    legend.text = element_text(size = 10)
    )

```

```{r}
ggplot(co2_zone3_2019_new, aes(x = TIMESTAMP, y = VALUE)) +
  geom_point(aes(color = Is_Outlier, shape = Is_Outlier), size = 0.75, alpha = 0.7) +  # Keep the point size small in the plot
  scale_color_manual(
    values = c("FALSE" = "pink", "TRUE" = "#D62728"),
    labels = c("Normal", "Outlier")
  ) +
  scale_shape_manual(
    values = c("FALSE" = 16, "TRUE" = 17),
    labels = c("Normal", "Outlier")
  ) +
  labs(
    title = "KNN Outlier Detection on CO2 Data 2019",
    x = "Timestamp",
    y = "CO2 Value",
    color = "Outliers",  # Change label in the legend
    shape = "Outliers"   # Change label in the legend
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    axis.text = element_text(color = "black"),
    axis.title = element_text(color = "black"),
    legend.background = element_rect(fill = "white", color = "gray70"),
    legend.position = "bottom",  # Move the legend to the bottom
    legend.box.margin = margin(t = 10),
    legend.text = element_text(size = 10),  # Increase legend text size
    legend.title = element_blank(),  # Remove the "Point Type" label
    legend.key.size = unit(1.5, "lines")  # Increase the size of the points in the legend
  )



```
```{r}
# Calculate the Q1, Q3, and IQR
Q1 <- quantile(co2_zone3_2019_new$VALUE, 0.25)
Q3 <- quantile(co2_zone3_2019_new$VALUE, 0.75)
IQR_value <- Q3 - Q1

# Define the outlier thresholds
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

# Detect outliers
co2_zone3_2019_new$Is_Outlier <- co2_zone3_2019_new$VALUE < lower_bound | co2_zone3_2019_new$VALUE > upper_bound

# Visualize the data with outliers
ggplot(co2_zone3_2019_new, aes(x = TIMESTAMP, y = VALUE)) +
  geom_point(aes(color = Is_Outlier, shape = Is_Outlier), size = 0.75, alpha = 1.5) +
  scale_color_manual(
    values = c("FALSE" = "pink", "TRUE" = "#D62728"),
    labels = c("Normal", "Outlier")
  ) +
  scale_shape_manual(
    values = c("FALSE" = 16, "TRUE" = 17),
    labels = c("Normal", "Outlier")
  ) +
  labs(
    title = "Outliers Detection Using Quartile Method",
    x = "Timestamp",
    y = "CO2 Value",
    color = "Outliers",  # Custom label in the legend
    shape = "Outliers"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    axis.text = element_text(color = "black"),
    axis.title = element_text(color = "black"),
    legend.background = element_rect(fill = "white", color = "gray70"),
    legend.position = "bottom",  # Move the legend to the bottom
    legend.box.margin = margin(t = 10),
    legend.text = element_text(size = 10),  # Increase legend text size
    legend.title = element_blank(),  # Remove the "Outliers" label
    legend.key.size = unit(1.5, "lines")  # Increase the size of the points in the legend
  )

```
### Relative Humidity for the North East Tower.

### Data Quality Summary: North East Tower Relative Humidity

Null Value Analysis:
- Complete dataset verification: No missing values found in any columns
- Column-wise validation: All 6 sensor columns show 0 null entries
- Full-table assessment: Total null count across entire dataset = 0

Key Findings:
1. The relative humidity dataset exhibits complete data integrity
2. All sensor readings (100m, 300m, 700m, 1300m, 2000m) recorded values at every timestamp
3. No gaps detected in the 15-minute interval measurements

Quality Assurance:
- Verified through:
  - Column-wise null summation (`colSums(is.na())`)
  - Full-table null count (`sum(is.na())`)
  - Visual inspection of temporal continuity
  
Identifying the data : Filters 2020 humidity data, verifies 15-minute reading completeness (96/day), and identifies dates with missing measurements.

```{r, northeast_tower}

## Reading the humidity data file by skipping the two rows and put the header as False for not considering the name of the column
humidity1 <- read.csv("data/NE_Tower_Relative humidity.csv", skip = 4, header = FALSE)

#Copying te original dataframe
ne_humidity <- data.frame(humidity1)


# Taking off the null column the V7 by selecting 6 columns only
ne_humidity <- ne_humidity[, 1:6]


# Reassigning the column names as needed
colnames(ne_humidity) <- c("DateTime", "TRF_NETower_100_HMP45", "TRF_NETower_300_HMP45","TRF_NETower_700_HMP45","TRF_NETower_1300_HMP45","TRF_NETower_2000_HMP45")


```

##Checking for the top 5 rows and also null values in the entire table and also for each column.
```{r, n_tower}

## Reading the first 5 rows
head(ne_humidity)

# Check for missing values in each column
colSums(is.na(ne_humidity))

##Missing values
sum(is.na(ne_humidity))

```

```{r, humidity}

## Filtering out the '2020' data from humidity dataframe
ne_humid_2020 <- ne_humidity |> filter(year(DateTime) == 2020)
                           
                           
##  Counting the number as the readings takes place for every 15 min
readings_per_day_humid <- ne_humid_2020 |> group_by(Date = date(DateTime)) %>% summarize(Readings = n())

##Missing date readings
missing_day_humid <- readings_per_day_humid |> filter(Readings != 96)

cat("Missing dates")
print(missing_day_humid)
```

```{r}
## Filtering out the '2019' data from humidity dataframe
ne_humid_2019 <- ne_humidity |> filter(year(DateTime) == 2019)

## Counting the number of readings that take place every 15 minutes
readings_per_day_humid_2019 <- ne_humid_2019 |> group_by(Date = date(DateTime)) %>% summarize(Readings = n())

## Missing date readings for 2019
missing_day_humid_2019 <- readings_per_day_humid_2019 |> filter(Readings != 96)

cat("Missing dates for 2019")
print(missing_day_humid_2019)

```


#### Sensor Calibration: Relative Humidity Corrections for NorthEast Tower with Relative Humidity.

Generated new columns for calibrated values at each tower height (100m, 300m, 700m, 1300m, 2000m)

```{r, ne_humidity}

#Generating new columns for the humidity dataframe
ne_humid_2020 <- ne_humid_2020 %>%
  mutate(
    RH_NE_100_Corr  = TRF_NETower_100_HMP45  * 1.038 + 1.0,
    RH_NE_300_Corr  = TRF_NETower_300_HMP45  * 0.986 - 2.1,
    RH_NE_700_Corr  = TRF_NETower_700_HMP45  * 1.055 - 6.8,
     RH_NE_1300_Corr = TRF_NETower_1300_HMP45 * 0.9213 + 5.6,
    RH_NE_2000_Corr = TRF_NETower_2000_HMP45 * 0.9889 + 3.0,
   )

# View the updated dataframe
head(ne_humid_2020)
```

```{r}
#Generating new columns for the humidity dataframe
ne_humid_2019 <- ne_humid_2019 %>%
  mutate(
    RH_NE_100_Corr  = TRF_NETower_100_HMP45  * 1.038 + 1.0,
    RH_NE_300_Corr  = TRF_NETower_300_HMP45  * 0.986 - 2.1,
    RH_NE_700_Corr  = TRF_NETower_700_HMP45  * 1.055 - 6.8,
     RH_NE_1300_Corr = TRF_NETower_1300_HMP45 * 0.9213 + 5.6,
    RH_NE_2000_Corr = TRF_NETower_2000_HMP45 * 0.9889 + 3.0,
   )

# View the updated dataframe
head(ne_humid_2019)
```
#### Performed the calculation as by taking the formula and it's the value for each column that is each sensor at particular heights.

```{r, ne_humidity_2020_mean}

# Select columns starting with "RH"
rh_columns <- grep("RH", names(ne_humid_2020), value = TRUE)

# Subset the data using the selected column names
rh_data <- ne_humid_2020[, rh_columns]

# Calculate the mean of those columns
ne_humid_mean <- colMeans(rh_data)

# Print the result
cat("Mean of the columns in the realtive humidity columns of the dataframe")
print(ne_humid_mean)

```

```{r}
# Select columns starting with "RH"
rh_columns_2019 <- grep("RH", names(ne_humid_2019), value = TRUE)

# Subset the data using the selected column names
rh_data_2019 <- ne_humid_2019[, rh_columns_2019]

# Calculate the mean of those columns
ne_humid_mean_2019 <- colMeans(rh_data_2019)

# Print the result
cat("Mean of the columns in the realtive humidity columns of the dataframe")
print(ne_humid_mean_2019)
```

#Outliers for the North East Tower of Relative Humidity
```{r, outliers, warning= FALSE}

# Select columns starting with "RH"
rh_columns <- grep("^RH", names(ne_humid_2020), value = TRUE)

# Considering values of those columns
humidity_rh <- ne_humid_2020[, rh_columns]

# Function to detect outliers using KNN
detect_outliers <- function(data) {
  
  # Compute KNN distances 
  knn_dist <- knn.dist(as.matrix(data), k = 6)
  
  # Calculate the 95th percentile of distances for each point
  quantile_distances <- apply(knn_dist, 1, function(x) quantile(x, 0.95))
  
  # Identify outliers as points where the distance exceeds the 95th percentile
  outliers <- quantile_distances > quantile(quantile_distances, 0.95)
  
  return(outliers)
}

# Initialize a list to store outlier results
outlier_results <- list()

# Detect outliers for each column
for (col in rh_columns) {
  column_data <- humidity_rh[[col]]
  outliers <- detect_outliers(column_data)
  outlier_results[[col]] <- outliers
}

# Visualize outliers for each column
for (col in rh_columns){
  # Create a data frame for plotting
  plot_data <- data.frame(
    Index = 1:nrow(humidity_rh),
    Value = humidity_rh[[col]],
    Outlier = outlier_results[[col]]
  )
  
  # Plot the data with outliers highlighted
  p<- ggplot(plot_data, aes(x = Index, y = Value, color = Outlier)) +
    geom_point(size = 2, alpha = 0.6) +
    scale_color_manual(values = c("TRUE"= "#5e66ad","FALSE" = "#42bd35" ),
                       labels = c("Outlier", "Normal"),
                       guide = guide_legend(shape = 16)) +
    labs(title = paste("Outliers in", col),
         x = "ObservationIndex",
         y = "Humidity Value") +
    theme_linedraw()+
    theme(
      plot.background = element_rect(colour = "blue", linewidth = 1),
      panel.background = element_rect(colour = "blue", linewidth = 1),
      plot.title = element_text(hjust = 0.6, face = "bold"),
      axis.title = element_text(size = 12),
      legend.background = element_rect(colour = "blue"),
      legend.title = element_text(hjust = 0.5, color = "black", face = "bold")
      )
  
  print(p)
}
  
```
  
```{r}
# Select columns starting with "RH"
rh_columns_2019 <- grep("^RH", names(ne_humid_2019), value = TRUE)

# Considering values of those columns
humidity_rh_2019 <- ne_humid_2019[, rh_columns_2019]

# Function to detect outliers using KNN
detect_outliers <- function(data, k = 6, quantile_threshold = 0.95) {
  
  # Compute KNN distances 
  knn_dist <- knn.dist(as.matrix(data), k = k)
  
  # Calculate the 95th percentile of distances for each point
  quantile_distances <- apply(knn_dist, 1, function(x) quantile(x, quantile_threshold))
  
  # Identify outliers as points where the distance exceeds the 95th percentile
  outliers <- quantile_distances > quantile(quantile_distances, quantile_threshold)
  
  return(outliers)
}

# Initialize a list to store outlier results
outlier_results <- list()

# Detect outliers for each column
for (col in rh_columns_2019) {
  column_data <- humidity_rh_2019[[col]]
  outliers <- detect_outliers(column_data)
  outlier_results[[col]] <- outliers
}

# Improved Visualization
for (col in rh_columns_2019) {
  # Create a data frame for plotting
  plot_data <- data.frame(
    Index = 1:nrow(humidity_rh_2019),
    Value = humidity_rh_2019[[col]],
    Outlier = outlier_results[[col]]
  )
  
  # Plot the data with outliers highlighted
  p <- ggplot(plot_data, aes(x = Index, y = Value, color = Outlier)) +
    geom_point(size = 0.75, alpha = 0.7) +  # Increased point size and adjusted alpha
    scale_color_manual(values = c("TRUE" = "#D62728", "FALSE" = "#1F77B4"),  # Updated color scheme for better contrast
                       labels = c("Outlier", "Normal")) +
    labs(
      title = paste("Outliers in", col),
      subtitle = "KNN Distance-based Outlier Detection",
      x = "Observation Index",
      y = "Humidity Value",
      color = "Outlier Status"
    ) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
      plot.subtitle = element_text(hjust = 0.5, color = "gray40", size = 12),
      axis.title = element_text(size = 14),
      axis.text = element_text(color = "black"),
      legend.position = "bottom",  # Move legend to bottom
      legend.title = element_text(face = "bold"),
      legend.background = element_rect(fill = "white", color = "gray70"),
      legend.text = element_text(size = 12)
    )
  
  print(p)
}

  
```




### Data Quality Summary: North East Tower Temperature data

Null Value Analysis: Complete dataset verification: No missing values found in any columns

Key Findings:
1. The relative temperature dataset exhibits complete data integrity
2. All sensor readings (100m, 300m, 700m, 1300m, 2000m) recorded values at every timestamp
3. No gaps detected in the 15-minute interval measurements

Quality Assurance:
Verified through:
  Column-wise null summation (colSums(is.na()))
  Full-table null count (sum(is.na()))

  
Identifying the data : Filters 2020 temperature data, verifies 15-minute reading completeness (96/day), and identifies dates with missing measurements.

# North East Temperature Calibrations
```{r, ne_temperature}

##Importing the temperature dataset
temperature1 <- read.csv('data/NE_TowerTemperature.csv', skip = 4, header = FALSE)

#Taking a copy of it
ne_temperature <- data.frame(temperature1)

# Taking off the null column the V7 by selecting 6 columns only
ne_temperature <- ne_temperature[, 1:6]


# Reassigning the column names as needed
colnames(ne_temperature) <- c("DateTime", "TRF_NETower_100_HMP45", "TRF_NETower_300_HMP45","TRF_NETower_700_HMP45","TRF_NETower_1300_HMP45","TRF_NETower_2000_HMP45")




```


#Reading the top 5 and missing values in NE - Temperature
```{r}
## Reading the first 5 rows
head(ne_temperature)

# Check for missing values in each column
colSums(is.na(ne_temperature))

##Missing values
sum(is.na(ne_temperature))

```

## The above code says that the file doesn't have any null values in the North East Tower - the temperature. Making sure twice on null values for the entire table and also in each of the column.
```{r, ne_temp_readings}


## Filtering out the '2020' data from temperature dataframe
ne_temp_2020 <- ne_temperature |> filter(year(DateTime) == 2020)
                           
                           
##  Counting the number as the readings takes place for every 15 min
readings_per_day_temp<- ne_temp_2020 |> group_by(Date = date(DateTime)) %>% summarize(Readings = n())

##Missing date readingsof the temperature
missing_day_temp <- readings_per_day_temp |> filter(Readings != 96)

cat("Missing dates on NE Temperature")
print(missing_day_temp)
```

This clarifies that there is no missing data in the Readings column which means they have recorded every single value through the sensors. So there is no need to perform any mean value to it.

#### Sensor Calibration: Temperature Corrections for NorthEast Tower with Relative Humidity.

Generated new columns for calibrated values at each tower height (100m, 300m, 700m, 1300m, 2000m)


```{r, ne-temp_calibrations}

#Generating new columns for the temperature dataframe

ne_temp_2020 <- ne_temp_2020 %>%
  mutate(
    T_NE_100_Corr  = TRF_NETower_100_HMP45 * 1.032 - 0.58,
    T_NE_300_Corr  = TRF_NETower_300_HMP45 * 0.9883 + 0.6049,
    T_NE_700_Corr  = TRF_NETower_700_HMP45 * 1.045 - 0.88,
    T_NE_1300_Corr = TRF_NETower_1300_HMP45 * 1.007  - 0.08,
    T_NE_2000_Corr = TRF_NETower_2000_HMP45 * 0.9988 -0.39
   )

# View the updated dataframe
head(ne_temp_2020)
```

#### Performed the calculation as by taking the formula and it's the value for each column that is each sensor at particular heights of North East tower of Temperature values.

```{r, average}

# Select columns starting with "T_NE"
t_columns <- grep("T_NE", names(ne_temp_2020), value = TRUE)

# Subset the data using the selected column names
t_data <- ne_temp_2020[, t_columns]

# Calculate the mean of those columns
ne_temp_mean <- colMeans(t_data)

# Print the result
cat("Mean of the columns in the temperature dataframe")
print(ne_temp_mean)

```


#Finally finding out the outliers of the NE-Temperature
```{r}

# Select the corrected columns starting with "T_NE"
temp_columns <- grep("^T_NE", names(ne_temp_2020), value = TRUE)

# Considering values of those columns
temp_df <- ne_temp_2020[, temp_columns]

# Function to detect outliers using KNN
detect_outlier_temp <- function(data) {
  
  # Compute KNN distances
  knn_dist_temp <- knn.dist(as.matrix(data), k = 6)
  
  # Calculate the 95th percentile of distances for each point
  quantile_dist_temp <- apply(knn_dist_temp, 1, function(x) quantile(x, 0.95))
  
  # Identify outliers as points where the distance exceeds the 95th percentile
  outlier_temp <- quantile_dist_temp > quantile(quantile_dist_temp, 0.95)
  
  return(outlier_temp)
}

# Initialize a list to store outlier results
outlier_results <- list()

# Detect outliers for each column
for (col in temp_columns) {
  column_data <- temp_df[[col]]
  outliers <- detect_outlier_temp(column_data)
  outlier_results[[col]] <- outliers
}

# Visualize outliers for each column
for (col in temp_columns) {  
  # Create a data frame for plotting
  plot_data <- data.frame(
    Index = 1:nrow(temp_df),  
    Value = temp_df[[col]],   
    Outlier = outlier_results[[col]]
  )
  
  # Plot the data with outliers highlighted
  p1 <- ggplot(plot_data, aes(x = Index, y = Value, fill = Outlier)) +
    geom_point(size = 2, shape = 21, stroke = 0.5, color = "#e0e0c5") +
    scale_fill_manual(values = c("#2e2b2c", "#d60f51"), labels = c("Normal", "Outlier")) +
    labs(title = paste("Outliers in", col),
         x = "Observation Index",
         y = "Temperature") +
    theme_gray()+
    theme(
      plot.background = element_rect(color = "black"),
      panel.background = element_rect(color = "black"),
      plot.title = element_text(face = "italic", hjust = 0.6),
      axis.title = element_text(face = "italic"),
      axis.text = element_text(face = 'italic'),
      legend.background = element_rect(color = "black"),
      legend.key = element_rect(color = 'black'),
      legend.text = element_text(face = 'italic')
      
      
    )
  
  print(p1)
}
```


#Each day value according to the zone wise




### South Tower Data Processing Summary  
1. Imported 15-minute resolution humidity/temperature data from South Tower sensors (100m–2000m elevations).  
2. Preprocessed by removing null columns, standardizing headers, and preserving raw data copies.  
3. Structured datasets with consistent columns: `DateTime` + 5 elevation-specific sensor readings.  
4. Verified data integrity—no missing values detected during initial inspection.  
5. Prepared for calibration and integration with other tower datasets for microclimate analysis.

```{r, warning=FALSE}

#Importing the other towers relative humidity and temperature for south, northwest and the mountain tower.
  
humidity2 <- read.csv('data/S_Tower_Relative humidity.csv', skip = 4, header = FALSE)

#taking a copy of the dataframe
s_humid <- data.frame(humidity2)

# Taking off the null column the V7 by selecting 6 columns only
s_humid <- s_humid[, 1:6]


# Reassigning the column names as needed
colnames(s_humid) <- c("DateTime", "TRF_STower_100_HMP45", "TRF_STower_300_HMP45","TRF_STower_700_HMP45","TRF_STower_1300_HMP45","TRF_SETower_2000_HMP45")


#Loading the file
temperature2 <- read.csv('C:/Users/ual-laptop/Desktop/NE/S_Tower_Temperature.csv', skip = 4, header = FALSE)

#Copying the dataframe
s_temp <- data.frame(temperature2)

# Taking off the null column the V7 by selecting 6 columns only
s_temp <- s_temp[, 1:6]

# Reassigning the column names as needed
colnames(s_temp) <- c("DateTime", "TRF_STower_100_HMP45", "TRF_STower_300_HMP45","TRF_STower_700_HMP45","TRF_STower_1300_HMP45","TRF_SETower_2000_HMP45")

```


### NORTH WEST - RELATIVE HUMIDITY AND TEMPERATURE
1. Imported 15-minute resolution humidity/temperature data from North West Tower sensors (100m–2000m elevations).  
2. Preprocessed by removing null columns, standardizing headers, and preserving raw data copies.  
3. Structured datasets with consistent columns: `DateTime` + 5 elevation-specific sensor readings.  
4. Verified data integrity—no missing values detected during initial inspection.  
5. Prepared for calibration and integration with other tower datasets for microclimate analysis.
```{r, nw_tower}

#Loading the file
temperature3 <- read.csv('data/NW_Tower_Temperature.csv', skip = 4, header = FALSE)

#Copying the file
nw_temp <- data.frame(temperature3)

# Taking off the null column the V6 by selecting 6 columns only
nw_temp <- nw_temp[, 1:5]

# Reassigning the column names as needed
colnames(nw_temp) <- c("DateTime", "TRF_NWTower_100_HMP45", "TRF_NWTower_300_HMP45","TRF_NWTower_700_HMP45","TRF_NWTower_1300_HMP45")

#Loading the file
humidity3 <- read.csv('data/NW_Tower_Relative humidity.csv', skip = 4, header = FALSE )

#Copying the file
nw_humid <- data.frame(humidity3)

# Taking off the null column the V6 by selecting 6 columns only
nw_humid <- nw_humid[, 1:5]

# Reassigning the column names as needed
colnames(nw_humid) <- c("DateTime", "TRF_NWTower_100_HMP45", "TRF_NWTower_300_HMP45","TRF_NWTower_700_HMP45","TRF_NWTower_1300_HMP45")

```


### MOUNTAIN TOWER - TEMPERATURE AND RELATIVE HUMIDITY
1. Imported 15-minute resolution humidity/temperature data from Mountain Tower sensors (100m–2000m elevations).  
2. Preprocessed by removing null columns, standardizing headers, and preserving raw data copies.  
3. Structured datasets with consistent columns: `DateTime` + 5 elevation-specific sensor readings.  
4. Verified data integrity—no missing values detected during initial inspection.  
5. Prepared for calibration and integration with other tower datasets for microclimate analysis.
```{r, mountain tower}

#loading the file
temperature4 <- read.csv('C:/Users/ual-laptop/Desktop/NE/Mtn_Tower_Temperature.csv', skip = 4, header = FALSE)

#Taking a copy of the file
mtn_temp <- data.frame(temperature4)


# Taking off the null column the V6 by selecting 6 columns only
mtn_temp <- mtn_temp[, 1:5]

# Reassigning the column names as needed
colnames(mtn_temp) <- c("DateTime", "TRF_MTNTower_100_HMP45", "TRF_MTNTower_300_HMP45","TRF_MTNTower_700_HMP45","TRF_MTNTower_1300_HMP45")


#Loading the file
humidity4 <- read.csv('C:/Users/ual-laptop/Desktop/NE/Mtn_Tower_Relative humidity.csv', skip = 4, header = FALSE )

#Copying the dataframe
mtn_humid <- data.frame(humidity4)


# Taking off the null column the V6 by selecting 6 columns only
mtn_humid <- mtn_humid[, 1:5]

# Reassigning the column names as needed
colnames(mtn_humid) <- c("DateTime", "TRF_MTNTower_100_HMP45", "TRF_MTNTower_300_HMP45","TRF_MTNTower_700_HMP45","TRF_MTNTower_1300_HMP45")

```


### Data Quality Validation

This code systematically checks for missing values across humidity and temperature datasets from three towers (Northwest, Mountain, South) using a custom `check_na_counts` function. It calculates and displays NA counts per column for all six datasets, enabling consistent data gap identification across the sensor network. The automated process ensures uniform quality assessment while maintaining clear dataset labeling.


```{r, nullvalues}


# Function to check for NA values in each column
check_na_counts <- function(df, name) {
  na_counts <- colSums(is.na(df))
  cat(paste("NA counts for", name, "\n"))
  print(na_counts)
  
  return(na_counts)
}

# Applying functions to all datasets
na_nw_humid <- check_na_counts(nw_humid, "North West Humidity")
na_nw_temp <- check_na_counts(nw_temp, "North West Temperature")
na_mtn_humid <- check_na_counts(mtn_humid, "Mountain Humidity")
na_mtn_temp <- check_na_counts(mtn_temp, "Mountain Temperature")
na_s_humid <- check_na_counts(s_humid, "Southern Humidity")
na_s_temp <- check_na_counts(s_temp, "Southern Temperature")


```

### Data Completeness Check
This code validates 2020 sensor data completeness across all towers by:
Filtering to 2020 data and counting daily readings (expected: 96/day)
Identifying dates with incomplete measurements. Processing both humidity and temperature data from three towers (NW, Mountain, South). Returning cleaned 2020 datasets while reporting gaps.


```{r, 2020 data}

# Function to filter and find missing readings

process_data <- function(tower, name) {
  tower_2020 <- tower |> filter(year(DateTime) == 2020)
  readings_per_day <- tower_2020 |> group_by(Date = date(DateTime)) |> summarize(Readings = n())
  missing_days <- readings_per_day |> filter(Readings != 96)
  
  cat(paste("Missing dates on", name, "\n"))
  print(missing_days)
  
  return(tower_2020)
}

# Applying the function to all datasets
nw_humid_2020 <- process_data(nw_humid, "NW Humidity")
nw_temp_2020 <- process_data(nw_temp, "NW Temperature")
mtn_humid_2020 <- process_data(mtn_humid, "Mountain Humidity")
mtn_temp_2020 <- process_data(mtn_temp, "Mountain Temperature")
s_humid_2020 <- process_data(s_humid, "Southern Humidity")
s_temp_2020 <- process_data(s_temp, "Southern Temperature")

```
### Calibrations of temperature and relative humidity sensors for the South Tower, North West Tower, and Mountain Tower.


### Conducting temperature and relative humidity sensor calibrations for the Mountain Tower.
```{r, mtn_tower}

#Temperature: Applied elevation-specific correction formulas to raw sensor data
mtn_temp_2020 <- mtn_temp_2020 %>%
  mutate(
    T_MTN_100_Corr  = TRF_MTNTower_100_HMP45,
    T_MTN_300_Corr  = TRF_MTNTower_300_HMP45 * 0.9383 + 1.67,
    T_MTN_700_Corr  = TRF_MTNTower_700_HMP45 * 1.129 - 2.4,
    T_MTN_1300_Corr = TRF_MTNTower_1300_HMP45 * 1.004 - 0.02
  )

# Relative Humidity: Corrected raw values with standardized transformation equation
mtn_humid_2020 <- mtn_humid_2020 %>%
  mutate(
    RH_MTN_100_Corr  = TRF_MTNTower_100_HMP45,
    RH_MTN_300_Corr  = TRF_MTNTower_300_HMP45 * 0.8975 + 12.2,
    RH_MTN_700_Corr  = TRF_MTNTower_700_HMP45 * 0.863 + 16.2,
    RH_MTN_1300_Corr = TRF_MTNTower_1300_HMP45 * 1.008 + 1.1
  )

```



#### Conducting temperature and relative humidity sensor calibrations for the North west Tower.
```{r, nwtower}

#Temperature: Applied elevation-specific correction formulas to raw sensor data
nw_temp_2020 <- nw_temp_2020 %>%
  mutate(
    T_NW_100_Corr  = TRF_NWTower_100_HMP45 * 0.9922 + 0.6,
    T_NW_300_Corr  = TRF_NWTower_300_HMP45 * 1.005 - 0.17,
    T_NW_700_Corr  = TRF_NWTower_700_HMP45 * 1.007 - 0.14,
    T_NW_1300_Corr = TRF_NWTower_1300_HMP45 * 0.9801 + 0.54
  )

#Relative Humidity: Corrected raw values with standardized transformation equation
nw_humid_2020 <- nw_humid_2020 %>%
  mutate(
    RH_NW_100_Corr  = TRF_NWTower_100_HMP45 * 0.9756 - 1.2,
    RH_NW_300_Corr  = TRF_NWTower_300_HMP45 * 0.9882 - 1.6,
    RH_NW_700_Corr  = TRF_NWTower_700_HMP45 * 0.9895 + 1.1,
    RH_NW_1300_Corr = TRF_NWTower_1300_HMP45 * 1.012 + 1.334
  )

```



#### Conducting temperature and relative humidity sensor calibrations for the South Tower.
```{r, s_tower}

#Temperature: Applied elevation-specific correction formulas to raw sensor data
s_temp_2020 <- s_temp_2020 %>%
  mutate(
    T_S_100_Corr  = TRF_STower_100_HMP45 * 1.041 + 0.08,
    T_S_300_Corr  = TRF_STower_300_HMP45 * 1.026 - 0.04,
    T_S_700_Corr  = TRF_STower_700_HMP45 * 1.037 - 0.15,
    T_S_1300_Corr = TRF_STower_1300_HMP45 * 1.059 - 0.31,
    T_S_2000_Corr = TRF_SETower_2000_HMP45  # No calibration needed
  )

#Relative Humidity: Corrected raw values with standardized transformation equation
s_humid_2020 <- s_humid_2020 %>%
  mutate(
    RH_S_100_Corr  = TRF_STower_100_HMP45 * 0.996 + 0.1,
    RH_S_300_Corr  = TRF_STower_300_HMP45 * 1.04 - 2.4,
    RH_S_700_Corr  = TRF_STower_700_HMP45 * 1.053 + 1.1,
    RH_S_1300_Corr = TRF_STower_1300_HMP45 * 0.9499 + 7.4,
    RH_S_2000_Corr = TRF_SETower_2000_HMP45  # No calibration needed
  )

```


### Zone wise calibrations

#### Zone -1 
Calculates daily mean temperature and relative humidity for Zone 1 (1m-3m elevation) by combining calibrated South Tower and NE Tower sensor data. Outputs preview of aggregated daily values showing:

Zone_1_Temp_Avg: Mean daily temperature across 100m/300m sensors
Zone_1_RH_Avg: Mean daily relative humidity across 100m/300m sensors




```{r}

zone1_data <- s_temp_2020 |>
  select(DateTime, T_S_100_Corr, T_S_300_Corr) %>%
  left_join(
    s_humid_2020 %>%
      select(DateTime, RH_S_100_Corr, RH_S_300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    ne_temp_2020 %>%
      select(DateTime, T_NE_100_Corr, T_NE_300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    ne_humid_2020 %>%
      select(DateTime, RH_NE_100_Corr, RH_NE_300_Corr),
    by = "DateTime"
  ) %>%
  mutate(Date = as.Date(DateTime, format = "%Y/%m/%d %H:%M")) %>%  
  group_by(Date) %>%
  summarize(
    Zone_1_Temp_Avg = mean(c(T_S_100_Corr, T_S_300_Corr, T_NE_100_Corr, T_NE_300_Corr), na.rm = TRUE),
    Zone_1_RH_Avg = mean(c(RH_S_100_Corr, RH_S_300_Corr, RH_NE_100_Corr, RH_NE_300_Corr), na.rm = TRUE),
    .groups = "drop"
  )

# View result
print("Zone 1 Data:")
print(head(zone1_data))



```


### Zone-2

Aggregates daily mean temperature and humidity for Zone 2 (7m-13m elevation) combining calibrated NW, South, and NE Tower sensor data. Output includes:

Zone 2 calculation (Temperature and Humidity from NW (100-300), South (700-1300), Northeast (700-1300))

Zone_2_Temp_Avg: Mean temperature (100m-1300m sensors)
Zone_2_RH_Avg: Mean relative humidity (100m-1300m sensors)


```{r}

# Zone 2 calculation (Temperature and Humidity from NW (100, 300, 700), South (700, 1300), Northeast (700,1300)) and Mountain Tower (100,300)

zone2_data <- nw_temp_2020 %>%
  select(DateTime, T_NW_100_Corr, T_NW_300_Corr, T_NW_700_Corr) %>%
  left_join(
    s_temp_2020 %>%
      select(DateTime, T_S_700_Corr, T_S_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    ne_temp_2020 %>%
      select(DateTime, T_NE_700_Corr, T_NE_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    mtn_temp_2020 %>%
      select(DateTime, T_MTN_100_Corr, T_MTN_300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    nw_humid_2020 %>%
      select(DateTime, RH_NW_100_Corr, RH_NW_300_Corr, RH_NW_700_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    s_humid_2020 %>%
      select(DateTime, RH_S_700_Corr, RH_S_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    ne_humid_2020 %>%
      select(DateTime, RH_NE_700_Corr, RH_NE_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    mtn_humid_2020 %>%
      select(DateTime, RH_MTN_100_Corr, RH_MTN_300_Corr),
    by = "DateTime"
  ) %>%
  mutate(Date = as.Date(DateTime, format = "%Y/%m/%d %H:%M")) %>%
  group_by(Date) %>%
  summarize(
    Zone_2_Temp_Avg = mean(c(
      T_NW_100_Corr, T_NW_300_Corr, T_NW_700_Corr,
      T_S_700_Corr, T_S_1300_Corr,
      T_NE_700_Corr, T_NE_1300_Corr,
      T_MTN_100_Corr, T_MTN_300_Corr
    ), na.rm = TRUE),
    
    Zone_2_RH_Avg = mean(c(
      RH_NW_100_Corr, RH_NW_300_Corr, RH_NW_700_Corr,
      RH_S_700_Corr, RH_S_1300_Corr,
      RH_NE_700_Corr, RH_NE_1300_Corr,
      RH_MTN_100_Corr, RH_MTN_300_Corr
    ), na.rm = TRUE),
    .groups = "drop"
  )

# View result
print("Zone 2 Data:")
print(head(zone2_data))


```
### Zone - 3

Calculates daily mean temperature and relative humidity for Zone 3 (13m-29m elevation) by combining calibrated South Tower NW Tower and NE sensor data. Outputs preview of aggregated daily values showing:

Zone_1_Temp_Avg: Mean daily temperature across (1300-2000) sensors
Zone_1_RH_Avg: Mean daily relative humidity across (1300-2000) sensors

Zone 3 calculation (Temperature and Humidity from South (2000), NW (1300), Mtn (1300-2000), Northeast (2000))

```{r, zone_3_data}

# Zone 3 calculation (Temperature and Humidity from South (2000), NW (1300), Mtn (700, 1300), Northeast (2000))

zone3_data <- s_temp_2020 %>%
  select(DateTime, T_S_2000_Corr) %>%
  left_join(
    nw_temp_2020 %>%
      select(DateTime, T_NW_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    mtn_temp_2020 %>%
      select(DateTime, T_MTN_700_Corr, T_MTN_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    ne_temp_2020 %>%
      select(DateTime, T_NE_2000_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    s_humid_2020 %>%
      select(DateTime, RH_S_2000_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    nw_humid_2020 %>%
      select(DateTime, RH_NW_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    mtn_humid_2020 %>%
      select(DateTime, RH_MTN_700_Corr, RH_MTN_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    ne_humid_2020 %>%
      select(DateTime, RH_NE_2000_Corr),
    by = "DateTime"
  ) %>%
  mutate(Date = as.Date(DateTime, format = "%Y/%m/%d %H:%M")) %>%
  group_by(Date) %>%
  summarize(
    Zone_3_Temp_Avg = mean(c(
      T_S_2000_Corr,
      T_NW_1300_Corr,
      T_MTN_700_Corr, T_MTN_1300_Corr,
      T_NE_2000_Corr
    ), na.rm = TRUE),
    
    Zone_3_RH_Avg = mean(c(
      RH_S_2000_Corr,
      RH_NW_1300_Corr,
      RH_MTN_700_Corr, RH_MTN_1300_Corr,
      RH_NE_2000_Corr
    ), na.rm = TRUE),
    
    .groups = "drop"
  )

# View result
print("Zone 3 Data:")
print(head(zone3_data))


```
## Zone wise calculation of no of moles

```{r, mole calculation}

#Loading the files for the zone1,zone2,zone3 of the carbondioxide files
co2_zone1 <- read.csv('data/SCADA_LOLNDCO2_1M.csv', skip = 2)
co2_zone2 <- read.csv('data/SCADA_LOLNDCO2_13M.csv', skip = 2)

#Already loaded the zone 3 for the carbondioxide
#co2_zone3 

```

```{r, null_values}

#Checking on the missing the value and fetching the mean of the carbondioxide for the year 2020  not considering the null values.

# Check for missing values in each column
colSums(is.na(co2_zone1))

# Check for missing values in each column
colSums(is.na(co2_zone2))

# Check for missing values in each column
colSums(is.na(co2_zone3))

```

### Handling Missing Carbondioxide Data Across Zones

Zones 1 and 2 had 14 missing carbondioxide values, which were addressed using linear interpolation to fill the gaps. Zone 3 required no further action, as interpolation had already been performed.

```{r, interpolation}

#Filling out the value's missing value with the interpolation
co2_zone1 <- co2_zone1 |> mutate(VALUE = na.approx(VALUE, na.rm = FALSE))
co2_zone2 <- co2_zone2 |> mutate(VALUE = na.approx(VALUE, na.rm = FALSE))

# Check for missing values in each column
colSums(is.na(co2_zone1))
colSums(is.na(co2_zone2))



```
```{r}

# Convert TIMESTAMP to a datetime format
co2_zone1 <- co2_zone1 %>% mutate(TIMESTAMP = as.POSIXct(TIMESTAMP, format = "%Y/%m/%d %H:%M:%S"))|> filter(year(TIMESTAMP) == 2020)
co2_zone2 <- co2_zone2 %>% mutate(TIMESTAMP = as.POSIXct(TIMESTAMP, format = "%Y/%m/%d %H:%M:%S"))|> filter(year(TIMESTAMP) == 2020)

                           
##  Counting the number as the readings takes place for every 15 min
readings_per_day_z1 <- co2_zone1 |> group_by(Date = date(TIMESTAMP)) %>% summarize(Readings = n())
readings_per_day_z2 <- co2_zone2 |> group_by(Date = date(TIMESTAMP)) %>% summarize(Readings = n())

##Missing date readings
missing_day_z1 <- readings_per_day_z1 |> filter(Readings != 96)
missing_day_z2 <- readings_per_day_z2 |> filter(Readings != 96)

cat("Missing dates of zone-1")
print(missing_day_z1)

cat("Missing dates of zone-2")
print(missing_day_z2)




```





### Zone-wise CO₂ Concentration Analysis (2020)

Computed the average daily carbon dioxide concentration for Zones 1, 2, and 3 in the year 2020 by filtering timestamps and averaging non-missing values.

```{r, daily_average}

# Calculate the daily average carbondioxide concentration for Zone 1 during the year 2020.
avg_co2_zone1 <- co2_zone1 |>
  mutate(Date = as.Date(TIMESTAMP),
         Year = year(Date)) |>
  filter(Year == 2020) |>                                           
  summarize(mean_co2 = mean(VALUE, na.rm = TRUE))|>              
  pull(mean_co2) 

# Calculate the daily average carbondioxide concentration for Zone 2 during the year 2020.
avg_co2_zone2 <- co2_zone2 |>
  mutate(Date = as.Date(TIMESTAMP),
         Year = year(Date)) |>
  filter(Year == 2020) |>                                           
  summarize(mean_co2 = mean(VALUE, na.rm = TRUE))|>              
  pull(mean_co2)


# Calculate the daily average Carbondioxide concentration for Zone 3 during the year 2020.
avg_co2_zone3 <- co2_zone3 |>
  mutate(Date = as.Date(TIMESTAMP),
         Year = year(Date)) |>
  filter(Year == 2020) |>                                           
  summarize(mean_co2 = mean(VALUE, na.rm = TRUE))|>              
  pull(mean_co2)

```

### Counting the number of reading for zone-1 and zone-3 of carbondioxide files.

### Checking if each day has 96 readings as we deal with co2_zone_3 values, and have inserted a row as there is a missing value on feb 25 at 9:30 because of some technical problems and it was restarted later.

```{r}

# Create new row for missing timestamp for the zone_1 carbondioxide
missingrow_z1 <- data.frame(ID = "0000", 
                          TIMESTAMP = as.POSIXct("2020-02-25 09:30:00"),
                          VALUE = avg_co2_zone1,
                          TRENDFLAGS = 0,  
                          STATUS = 0,
                          TRENDFLAGS_TAG = "{}",
                          STATUS_TAG = "{ok}"
  
)

# Insert into dataset and arranging the time stamp according to the time.
co2_zone1 <- co2_zone1 |>
  bind_rows(missingrow_z1) |>
  arrange(TIMESTAMP) 


# Create new row for missing timestamp for the zone_2 carbondioxide
missingrow_z2 <- data.frame(ID = "0000", 
                          TIMESTAMP = as.POSIXct("2020-02-25 09:30:00"),
                          VALUE = avg_co2_zone2,
                          TRENDFLAGS = 0,  
                          STATUS = 0,
                          TRENDFLAGS_TAG = "{}",
                          STATUS_TAG = "{ok}"
  
)

# Insert into dataset and arranging the time stamp according to the time.
co2_zone2 <- co2_zone2 |>
  bind_rows(missingrow_z2) |>
  arrange(TIMESTAMP)
```



### Zone-wise Temperature Averaging and Conversion (2020)

Calculated the average temperature for each zone in 2020 and converted the values from Celsius to Kelvin by adding 273.15.

```{r, temperature average zonewise}


#Averaging the temperature zonewise - 2020 and converting from celsius to kelvin by adding 273.15

temp_zone1 <- mean(zone1_data$Zone_1_Temp_Avg) + 273.15
temp_zone2 <- mean(zone2_data$Zone_2_Temp_Avg) + 273.15
temp_zone3 <- mean(zone3_data$Zone_3_Temp_Avg) + 273.15


```


#Calculating carbondioxide moles per zone

###### P - Pressure (Pa) from 884 mbar, V - Rainforest volume (m³) ,R- Gas constant (m³·Pa·K⁻¹·mol⁻¹) and Zone 1/2/3 volume fraction.
```{r, moles}

# Constants
P <- 88400  
V <- 26700    
R <- 8.31446   
vol_fracs <- c(0.304, 0.588, 0.108)  


# Function to compute Co2 moles for a zone
co2_moles <- function(avg_co2_ppm, temp_k, vol_frac) {
  n_air <- (P * V) / (R * temp_k)         
  n_co2 <- n_air * (avg_co2_ppm / 1e6) * vol_frac
  return(n_co2)
}


# Add daily CO2 moles to each zone's data
zone1_moles <- co2_moles(avg_co2_zone1,temp_zone1, vol_fracs[1])
zone2_moles <- co2_moles(avg_co2_zone2,temp_zone2, vol_fracs[2])
zone3_moles <- co2_moles(avg_co2_zone3,temp_zone3, vol_fracs[3])
  
print(paste("zone1 moles of co2:", zone1_moles, "zone-2 moles of co2:", zone2_moles, "zone-3 moles of co2:", zone3_moles))
```


### Calculating daily co2 moles and it's difference

To analyze daily carbondioxide dynamics across the rainforest zones, we first process the raw sensor data to calculate daily average CO₂ concentrations for each vertical zone (1m, 13m, and 27m) throughout 2020. This involves creating three separate datasets (co2_z1, co2_z2, co2_z3) where we: (1) convert timestamps to dates, (2) filter for 2020 measurements, and (3) compute daily mean CO₂ values while handling any missing data. These daily CO₂ averages are then joined to their corresponding zone datasets (zone1_data, zone2_data, zone3_data) which already contain the daily temperature and humidity measurements.

For accurate gas calculations, we convert all temperatures from Celsius to Kelvin by adding 273.15 to each daily temperature value. Using the ideal gas law (PV = nRT), we then calculate the daily moles of CO₂ in each zone through a dedicated function that: (1) first determines the total moles of air present using n_air = (P × V)/(R × T), where P is pressure (88400 Pa), V is zone volume fraction (26,700 m³ × zone fraction), R is the gas constant (8.314 J/mol·K), and T is temperature in Kelvin; then (2) calculates CO₂ moles by multiplying the total air moles by the CO₂ concentration (in ppm/1,000,000) and the zone's volume fraction (0.304, 0.588, or 0.108).

Finally, to track daily CO₂ fluctuations, we compute the difference iN CO₂ moles between consecutive days for each zone, storing these values in new co2_diff columns. This sequential differencing (day 2 - day 1, day 3 - day 2, etc.) reveals the net daily changes in CO₂ quantities at each canopy level, providing insights into vertical carbon flux patterns throughout the observation period. The complete dataset now enables both daily snapshots and trend analysis of the rainforest's carbon dynamics across its vertical profile.

```{r, day_wise_moles}

# Creating a new dataframe co2_z1 for each day's carbondioxide value
co2_z1 <- co2_zone1|> 
  mutate(Date = as.Date(TIMESTAMP), 
         Year = year(Date)) |>
  filter(Year == 2020) |> 
  group_by(Date) |>
  summarize(carbondioxide_z1 = mean(VALUE, na.rm = TRUE))


# Creating a new dataframe co2_z2 for each day's carbondioxide value
co2_z2 <- co2_zone2 |> 
  mutate(Date = as.Date(TIMESTAMP),Year = year(Date)) |>
  filter(Year == 2020) |> 
  group_by(Date) |>
  summarize(carbondioxide_z2 = mean(VALUE, na.rm = TRUE))


co2_z3 <- co2_zone3 |>
  mutate( Date = as.Date(TIMESTAMP),Year = year(Date)) |>
  filter(TIMESTAMP >= "2020/01/01 00:00:00") |>
  group_by(Date)|> 
  summarize(carbondioxide_z3 = mean(VALUE, na.rm = TRUE)) 



# Adding the carbondioxide value from co2_z1 dataframe to the zone1_data dataframe 
zone1_data <- zone1_data |>
  left_join(co2_z1, by = "Date") 



# Adding the carbondioxide value from co2_z2 dataframe to the zone2_data dataframe 
zone2_data <- zone2_data |>
  left_join(co2_z2, by = "Date") 


# Adding the carbondioxide value from co2_z1 dataframe to the zone1_data dataframe 
zone3_data <- zone3_data |>
  left_join(co2_z3, by = "Date") 


```




```{r}

#Adding temp with 273K to convert it to kelvin
zone1_data$Zone_1_Temp_Avg <- (zone1_data$Zone_1_Temp_Avg) + 273.15
zone2_data$Zone_2_Temp_Avg <- (zone2_data$Zone_2_Temp_Avg) + 273.15
zone3_data$Zone_3_Temp_Avg <- (zone3_data$Zone_3_Temp_Avg) + 273.15



# Creating a new column named moles in the zone1_data
zone1_data <- zone1_data |>
  mutate(moles = co2_moles(carbondioxide_z1, Zone_1_Temp_Avg, vol_fracs[1]))

# Creating a new column named moles in the zone2_data
zone2_data <- zone2_data |>
  mutate(moles = co2_moles(carbondioxide_z2, Zone_2_Temp_Avg, vol_fracs[2]))

# Creating a new column named moles in the zone3_data
zone3_data <- zone3_data |>
  mutate(moles = co2_moles(carbondioxide_z3, Zone_3_Temp_Avg, vol_fracs[3]))



#Zone 1,2,3 difference of moles calculated between consective dates
zone1_data$Zone_1_CO2_Diff <- c(NA, diff(zone1_data$moles))
zone2_data$Zone_2_CO2_Diff <- c(NA, diff(zone2_data$moles))
zone3_data$Zone_3_CO2_Diff <- c(NA, diff(zone3_data$moles))

```




```{r}

## Every 15 min no of moles and the difference between present and the next 15 min.
zone1_minute_data <- co2_zone1 |> mutate(TIMESTAMP = as.POSIXct(TIMESTAMP, format = "%Y/%m/%d %H:%M:%S"))|> filter(year(TIMESTAMP) == 2020) 


zone2_minute_data <- co2_zone2 |> mutate(TIMESTAMP = as.POSIXct(TIMESTAMP, format = "%Y/%m/%d %H:%M:%S"))|> filter(year(TIMESTAMP) == 2020) 

zone3_minute_data <- co2_zone3 |> mutate(TIMESTAMP = as.POSIXct(TIMESTAMP, format = "%Y/%m/%d %H:%M:%S"))|> filter(year(TIMESTAMP) == 2020)

```



```{r}

# Prepare Zone 1 temp and humidity
zone1_temp_humid <- s_temp_2020 %>%
  select(DateTime, T_S_100_Corr, T_S_300_Corr) %>%
  left_join(
    ne_temp_2020 %>%
      select(DateTime, T_NE_100_Corr, T_NE_300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    s_humid_2020 %>%
      select(DateTime, RH_S_100_Corr, RH_S_300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    ne_humid_2020 %>%
      select(DateTime, RH_NE_100_Corr, RH_NE_300_Corr),
    by = "DateTime"
  ) %>%
  mutate(
    TIMESTAMP = as.POSIXct(DateTime, format = "%Y/%m/%d %H:%M"),
    # Compute average temp and RH 
    Temp_Avg = rowMeans(select(., T_S_100_Corr, T_S_300_Corr, T_NE_100_Corr, T_NE_300_Corr), na.rm = TRUE),
    RH_Avg = rowMeans(select(., RH_S_100_Corr, RH_S_300_Corr, RH_NE_100_Corr, RH_NE_300_Corr), na.rm = TRUE)
  ) %>%
  select(TIMESTAMP, Temp_Avg, RH_Avg)
# Prepare Zone 2 CO₂ minute data
zone2_minute_data <- co2_zone2 %>%
  mutate(TIMESTAMP = as.POSIXct(TIMESTAMP, format = "%Y/%m/%d %H:%M:%S")) %>%
  filter(year(TIMESTAMP) == 2020)
```



```{r}
# Prepare Zone 2 temperature and humidity
zone2_temp_humid <- nw_temp_2020 %>%
  select(DateTime, T_NW_100_Corr, T_NW_300_Corr, T_NW_700_Corr) %>%
  left_join(
    s_temp_2020 %>% select(DateTime, T_S_700_Corr, T_S_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    ne_temp_2020 %>% select(DateTime, T_NE_700_Corr, T_NE_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    mtn_temp_2020 %>% select(DateTime, T_MTN_100_Corr, T_MTN_300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    nw_humid_2020 %>% select(DateTime, RH_NW_100_Corr, RH_NW_300_Corr, RH_NW_700_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    s_humid_2020 %>% select(DateTime, RH_S_700_Corr, RH_S_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    ne_humid_2020 %>% select(DateTime, RH_NE_700_Corr, RH_NE_1300_Corr),
    by = "DateTime"
  ) %>%
  left_join(
    mtn_humid_2020 %>% select(DateTime, RH_MTN_100_Corr, RH_MTN_300_Corr),
    by = "DateTime"
  ) %>%
  mutate(
    TIMESTAMP = as.POSIXct(DateTime, format = "%Y/%m/%d %H:%M"),
    Temp_Avg = rowMeans(select(., starts_with("T_")), na.rm = TRUE),
    RH_Avg = rowMeans(select(., starts_with("RH_")), na.rm = TRUE)
  ) %>%
  select(TIMESTAMP, Temp_Avg, RH_Avg)

```

```{r}
zone3_temp_humid <- ne_temp_2020 %>% select(DateTime, T_NE_2000_Corr) %>% 
  left_join(
    s_temp_2020 %>% select(DateTime, T_S_2000_Corr), by = "DateTime"
  )%>%
  left_join(
    nw_temp_2020 %>% select(DateTime, T_NW_1300_Corr), by = "DateTime"
  ) %>%
  left_join(
    mtn_temp_2020 %>% select(DateTime, T_MTN_700_Corr, T_MTN_1300_Corr), by = 'DateTime'
  ) %>%
  left_join(
    s_humid_2020 %>% select(DateTime, RH_S_2000_Corr), by = "DateTime"
  ) %>%
  left_join(
    nw_humid_2020 %>% select(DateTime, RH_NW_1300_Corr), by = "DateTime"
  ) %>%
  left_join(
    ne_humid_2020 %>% select(DateTime, RH_NE_2000_Corr), by = "DateTime"
  ) %>%
  left_join(
    mtn_humid_2020 %>% select(DateTime, RH_MTN_700_Corr, RH_MTN_1300_Corr), by = "DateTime"
  ) %>%
  mutate(
    TIMESTAMP = as.POSIXct(DateTime, format= "%Y/%m/%d %H:%M"),
    Temp_Avg = rowMeans(select(.,starts_with("T_")), na.rm = TRUE),
    Humid_Avg = rowMeans(select(., starts_with("RH_")), na.rm = TRUE)
  ) %>%
  select(TIMESTAMP, Temp_Avg, Humid_Avg)

```


```{r}
zone1_minute_data <- zone1_temp_humid %>%
  left_join(zone1_minute_data, by = "TIMESTAMP")


zone2_minute_data <- zone2_temp_humid %>%
  left_join(zone2_minute_data, by = "TIMESTAMP")

zone3_minute_data <- zone3_temp_humid %>%
  left_join(zone3_minute_data, by = "TIMESTAMP")

```


```{r}

zone1_minute_data <- zone1_minute_data |> mutate(Temp_Avg = Temp_Avg + 273.15)
zone2_minute_data <- zone2_minute_data |> mutate(Temp_Avg = Temp_Avg + 273.15)
zone3_minute_data <- zone3_minute_data |> mutate(Temp_Avg = Temp_Avg + 273.15)

```


```{r}
zone1_minute_data <- zone1_minute_data |> mutate(moles = co2_moles(VALUE, Temp_Avg, vol_fracs[1]))

zone2_minute_data <- zone2_minute_data |> mutate(moles = co2_moles(VALUE, Temp_Avg, vol_fracs[2]))

zone3_minute_data <- zone3_minute_data |> mutate(moles = co2_moles(VALUE, Temp_Avg, vol_fracs[3]))
```


### Calculating Differences for Each Zone
This code calculates the difference between consecutive moles values for each zone
```{r}
zone1_minute_data$diff_moles <- c(NA, diff(zone1_minute_data$moles)) 
zone2_minute_data$diff_moles <- c(NA, diff(zone2_minute_data$moles)) 
zone3_minute_data$diff_moles <- c(NA, diff(zone3_minute_data$moles)) 
```



###  Merging Data from All Three Zones
This merged minute moles dataframe selects the relevant columns from each zone (timestamp and diff_moles), renames them for clarity, and merges them into one data frame. Finally, it calculates the total mole difference across all three zones for every 15 min in the year of 2020 from Jan to Mar each day with 96 readings.

```{r}

# From all three zones selecting the time stamp and mole difference column
zone1_min_sub <- zone1_minute_data[, c("TIMESTAMP", "diff_moles")]
zone2_min_sub <- zone2_minute_data[, c("TIMESTAMP", "diff_moles")]
zone3_min_sub <- zone3_minute_data[, c("TIMESTAMP", "diff_moles")]

# Renaming the column for each zone's moles difference
colnames(zone1_min_sub)[2] <- "diff_moles_z1"
colnames(zone2_min_sub)[2] <- "diff_moles_z2"
colnames(zone3_min_sub)[2] <- "diff_moles_z3"

# Merge all by timestamp
merged_min_moles <- Reduce(function(x, y) merge(x, y, by = "TIMESTAMP", all = TRUE),
                 list(zone1_min_sub, zone2_min_sub, zone3_min_sub))

# Sum the differences across zones
merged_min_moles$total_diff_moles <- rowSums(merged_min_moles[, c("diff_moles_z1", "diff_moles_z2", "diff_moles_z3")], na.rm = TRUE)



```

